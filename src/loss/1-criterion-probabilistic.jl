## probabilistic loss

export crossEntropy
export crossEntropyLoss
export crossEntropyCost

export binaryCrossEntropy
export binaryCrossEntropyLoss
export binaryCrossEntropyCost


"""
    crossEntropy(x::Variable{T}, label::Variable{T}) -> Variable{T}
cross entropy = - y * log(Ì‚y) where y is target and Ì‚y is the output of the network.
"""
function crossEntropy(x::Variable{T}, label::Variable{T}) where T
    @assert (x.shape == label.shape)
    backprop = (x.backprop || label.backprop)
    Ïµ = eltype(x)(1e-38)
    y = Variable{T}(- áµ›(label) .* log.(áµ›(x) .+ Ïµ), backprop)
    if backprop
        function crossEntropyBackward()
            if need2computeÎ´!(x)
                Î´(x) .-= Î´(y) .* áµ›(label) ./ (áµ›(x) .+ Ïµ)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y);
        end
        push!(graph.backward, crossEntropyBackward)
    end
    return y
end

crossEntropyLoss(x::Variable{T}, label::Variable{T}) where T = loss( crossEntropy(x, label) )
crossEntropyCost(x::Variable{T}, label::Variable{T}) where T = cost( crossEntropy(x, label) )


"""
    binaryCrossEntropy(x::Variable{T}, l::Variable{T}) -> Variable{T}
binary cross entropy = - y * log(Ì‚y) - (1 - y) * log(1-Ì‚y)
"""
function binaryCrossEntropy(x::Variable{T}, label::Variable{T}) where T
    @assert (x.shape == label.shape)
    backprop = (x.backprop || label.backprop)
    TOO  = eltype(x)
    Ïµ  = TOO(1e-38)
    ğŸ™  = TOO(1.0f0)
    tmp1 = - áµ›(label) .* log.(áµ›(x) .+ Ïµ)
    tmp2 = - (ğŸ™ .- áµ›(label)) .* log.(ğŸ™ .- áµ›(x) .+ Ïµ)
    y  = Variable{T}(tmp1 + tmp2, backprop)
    if backprop
        function binaryCrossEntropyBackward()
            if need2computeÎ´!(x)
                temp1 = (ğŸ™ .- áµ›(label)) ./ (ğŸ™ .- áµ›(x) .+ Ïµ)
                temp2 = áµ›(label) ./ (áµ›(x) .+ Ïµ)
                Î´(x) .+= Î´(y) .* (temp1 - temp2)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y);
        end
        push!(graph.backward, binaryCrossEntropyBackward)
    end
    return y
end

binaryCrossEntropyLoss(x::Variable{T}, label::Variable{T}) where T = loss( binaryCrossEntropy(x, label) )
binaryCrossEntropyCost(x::Variable{T}, label::Variable{T}) where T = cost( binaryCrossEntropy(x, label) )
