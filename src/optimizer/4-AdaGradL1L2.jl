"""
    AdaGrad(::Vector{XVariable}; lr=1e-2, eps=1e-10, L1decay=0.001, L2decay=0.01)

Implements Adagrad algorithm. Refer to `Adaptive Subgradient Methods for Online Learning and Stochastic`
"""
mutable struct AdaGrad <: Optimizer
    xparams::Vector{XVariable}
    w::Vector
    lr::AbstractFloat
    Ïµ::AbstractFloat
    L1decay::AbstractFloat
    L2decay::AbstractFloat
    name::String
    function AdaGrad(xparams::Vector{XVariable}; lr=1e-2, eps=1e-10, L1decay=0.001, L2decay=0.01)
        n = length(xparams)
        w = Vector(undef,n)
        for i = 1:n
            c , Î¸ = xparams[i]
            w[i] = Zeros(typeof(áµ›(Î¸)), Î¸.shape)
        end
        new(xparams, w, lr, eps, L1decay, L2decay, "AdaGrad")
    end
end


function Base.show(io::IO, O::AdaGrad)
    print("AdaGrad(lr=$(O.lr), Ïµ=$(O.Ïµ), L1decay=$(O.L1decay), L2decay=$(O.L2decay))")
end


function update!(O::AdaGrad; clipfn::Function=LPInfNormClip, clipvalue=10.0)
    w = O.w
    Î¼ = - O.lr
    Ïµ = O.Ïµ
    Î»â‚ = O.L1decay
    Î»â‚‚ = O.L2decay

    for i = 1:length(O.xparams)
        c , Î¸ = O.xparams[i]
        âˆ‡ = clipfn(setNanInfZero(Î´(Î¸)), clipvalue)
        ð’— = áµ›(Î¸)
        @. w[i] += âˆ‡ * âˆ‡
        if c == 'w'
            if Î»â‚==0 && Î»â‚‚==0
                @. ð’— += Î¼ / (sqrt(w[i]) + Ïµ) * âˆ‡
            else if Î»â‚==0 && Î»â‚‚!=0
                @. ð’— += Î¼ / (sqrt(w[i]) + Ïµ) * (âˆ‡ + Î»â‚‚ * ð’—)
            else if Î»â‚!=0 && Î»â‚‚==0
                @. ð’— += Î¼ / (sqrt(w[i]) + Ïµ) * (âˆ‡ + Î»â‚ * sign(ð’—))
            else  # Î»â‚!=0 && Î»â‚‚!=0
                @. ð’— += Î¼ / (sqrt(w[i]) + Ïµ) * (âˆ‡ + Î»â‚ * sign(ð’—) + Î»â‚‚ * ð’—)
            end
        else
            @. ð’— += Î¼ / (sqrt(w[i]) + Ïµ) * âˆ‡
        end
    end
end
